{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d380843b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\negia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\negia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n",
      "Class weights: {0: 0.8766697374481806, 1: 1.1637114032405993}\n",
      "Epoch 1/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.6109 - loss: 0.6495 - val_accuracy: 0.7479 - val_loss: 0.5088\n",
      "Epoch 2/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.8692 - loss: 0.3653 - val_accuracy: 0.7898 - val_loss: 0.4941\n",
      "Epoch 3/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9389 - loss: 0.2316 - val_accuracy: 0.7775 - val_loss: 0.6260\n",
      "Epoch 4/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9756 - loss: 0.0898 - val_accuracy: 0.7750 - val_loss: 0.9435\n",
      "Epoch 5/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9870 - loss: 0.0430 - val_accuracy: 0.7586 - val_loss: 1.1285\n",
      "Epoch 6/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.9910 - loss: 0.0302 - val_accuracy: 0.7726 - val_loss: 1.3531\n",
      "Epoch 7/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9957 - loss: 0.0151 - val_accuracy: 0.7381 - val_loss: 2.4537\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
      "Result: DisasterRelated\n",
      "Score: 0.7312484383583069\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.utils import pad_sequences\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import nltk\n",
    "# import re\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# # Preprocessing function\n",
    "# def preprocess_text(text):\n",
    "#     lemma = nltk.WordNetLemmatizer()\n",
    "#     text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
    "#     text = ' '.join([lemma.lemmatize(word) for word in text.split() if word not in nltk.corpus.stopwords.words('english')])\n",
    "#     return text\n",
    "\n",
    "# # Load dataset\n",
    "# df = pd.read_csv('trainDisaster.csv')\n",
    "\n",
    "# # Fill missing values\n",
    "# df['text'] = df['text'].apply(preprocess_text)\n",
    "# df['keyword'] = df['keyword'].fillna('unknown')\n",
    "# df['location'] = df['location'].fillna('unknown')\n",
    "\n",
    "\n",
    "# print(df['target'].value_counts())\n",
    "\n",
    "# df['target'] = df['target'].astype(int)\n",
    "\n",
    "# # Prepare tokenizer and word sequences\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(df['text'])\n",
    "# word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "# X = tokenizer.texts_to_sequences(df['text'])\n",
    "# y = df['target']\n",
    "\n",
    "# # Compute class weights\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "# class_weights = dict(enumerate(class_weights))\n",
    "# print(\"Class weights:\", class_weights)\n",
    "\n",
    "\n",
    "# max_len = 30\n",
    "# X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=len(word_index)+1, output_dim=100))\n",
    "# model.add(LSTM(200, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# earlystopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2,  callbacks=[earlystopping])\n",
    "\n",
    "# # Save the trained model\n",
    "# # model.save('disaster_rnn_model.h5')\n",
    "\n",
    "# def preprocess_input(news):\n",
    "#     words = news.lower().split()\n",
    "#     encoded_review = [word_index.get(word, 2) for word in words]  # Using '2' for out-of-vocabulary words\n",
    "#     padded_review = pad_sequences([encoded_review], maxlen=max_len)\n",
    "#     return padded_review\n",
    "\n",
    "# def predict_news(news):\n",
    "#     preprocessed_text = preprocess_input(news)\n",
    "#     prediction = model.predict(preprocessed_text)\n",
    "#     sentiment = 'DisasterRelated' if prediction[0][0] > 0.5 else 'Not Related'\n",
    "#     return sentiment, prediction[0][0]\n",
    "\n",
    "\n",
    "# news = \"earthquake shakes the city, people are trapped\"\n",
    "# result, score = predict_news(news)\n",
    "# print(f'Result: {result}')\n",
    "# print(f'Score: {score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78eef52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Result: DisasterRelated\n",
      "Score: 0.960098147392273\n"
     ]
    }
   ],
   "source": [
    "news = \"In 2022, flooding and landslides in the northeastern state of Assam killed at least 192 people|\"\n",
    "result, score = predict_news(news)\n",
    "print(f'Result: {result}')\n",
    "print(f'Score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c98e45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting redis\n",
      "  Downloading redis-5.2.0-py3-none-any.whl.metadata (9.1 kB)\n",
      "Downloading redis-5.2.0-py3-none-any.whl (261 kB)\n",
      "   ---------------------------------------- 0.0/261.4 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/261.4 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/261.4 kB 445.2 kB/s eta 0:00:01\n",
      "   ------------------ --------------------- 122.9/261.4 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  256.0/261.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  256.0/261.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 261.4/261.4 kB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: redis\n",
      "Successfully installed redis-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bfc246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\negia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\negia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution:\n",
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n",
      "Class weights: {0: 0.8766697374481806, 1: 1.1637114032405993}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,140,800</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">467,968</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m200\u001b[0m)             │       \u001b[38;5;34m4,140,800\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │         \u001b[38;5;34m467,968\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m197,120\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,806,017</span> (18.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,806,017\u001b[0m (18.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,806,017</span> (18.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,806,017\u001b[0m (18.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 89ms/step - accuracy: 0.6621 - loss: 0.6095 - val_accuracy: 0.7923 - val_loss: 0.4498\n",
      "Epoch 2/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 86ms/step - accuracy: 0.9072 - loss: 0.2408 - val_accuracy: 0.7775 - val_loss: 0.5796\n",
      "Epoch 3/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 84ms/step - accuracy: 0.9755 - loss: 0.0773 - val_accuracy: 0.7644 - val_loss: 0.8183\n",
      "Epoch 4/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 86ms/step - accuracy: 0.9863 - loss: 0.0475 - val_accuracy: 0.7471 - val_loss: 0.8924\n",
      "Epoch 5/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 85ms/step - accuracy: 0.9940 - loss: 0.0247 - val_accuracy: 0.7521 - val_loss: 1.0152\n",
      "Epoch 6/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 91ms/step - accuracy: 0.9935 - loss: 0.0207 - val_accuracy: 0.7529 - val_loss: 1.1461\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 558ms/step\n",
      "Result: DisasterRelated\n",
      "Score: 0.92\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.utils import pad_sequences\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import nltk\n",
    "# import re\n",
    "\n",
    "# # Ensure necessary NLTK resources are downloaded\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# # Preprocessing function\n",
    "# def preprocess_text(text):\n",
    "#     lemma = nltk.WordNetLemmatizer()\n",
    "#     text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
    "#     text = ' '.join([lemma.lemmatize(word) for word in text.split() if word not in nltk.corpus.stopwords.words('english')])\n",
    "#     return text\n",
    "\n",
    "# # Load dataset\n",
    "# df = pd.read_csv('trainDisaster.csv')\n",
    "\n",
    "# # Fill missing values\n",
    "# df['text'] = df['text'].apply(preprocess_text)\n",
    "# df['keyword'] = df['keyword'].fillna('unknown')\n",
    "# df['location'] = df['location'].fillna('unknown')\n",
    "\n",
    "# # Check target distribution\n",
    "# print(\"Target distribution:\")\n",
    "# print(df['target'].value_counts())\n",
    "\n",
    "# # Convert target to integers\n",
    "# df['target'] = df['target'].astype(int)\n",
    "\n",
    "# # Prepare tokenizer and word sequences\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(df['text'])\n",
    "# word_index = tokenizer.word_index\n",
    "\n",
    "# # Prepare input data (X) and target data (y)\n",
    "# X = tokenizer.texts_to_sequences(df['text'])\n",
    "# y = df['target']\n",
    "\n",
    "# # Compute class weights\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "# class_weights = dict(enumerate(class_weights))\n",
    "# print(\"Class weights:\", class_weights)\n",
    "\n",
    "# # Pad sequences\n",
    "# max_len = 30\n",
    "# X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "# # Split dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Build the RNN model with LSTM layers\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=len(word_index)+1, output_dim=200)) \n",
    "# model.build(input_shape=(None, max_len))# Using 200-dimensional embeddings\n",
    "# model.add(LSTM(256, return_sequences=True))  # First LSTM layer with 256 units\n",
    "# model.add(Dropout(0.3))  # Dropout to prevent overfitting\n",
    "# model.add(LSTM(128))  # Second LSTM layer with 128 units\n",
    "# model.add(Dropout(0.3))  # Additional Dropout layer\n",
    "# model.add(Dense(1, activation='sigmoid'))  # Binary classification with sigmoid activation\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Binary loss and optimizer\n",
    "\n",
    "# # Display model summary\n",
    "# model.summary()\n",
    "\n",
    "# # Early stopping callback\n",
    "# earlystopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# # Train the model with class weights\n",
    "# model.fit(X_train, y_train, \n",
    "#           epochs=20, \n",
    "#           batch_size=32, \n",
    "#           validation_split=0.2, \n",
    "# #           class_weight=class_weights, \n",
    "#           callbacks=[earlystopping])\n",
    "\n",
    "# # Save the trained model\n",
    "# # model.save('disaster_rnn_model.h5')\n",
    "\n",
    "# # Function to preprocess input news\n",
    "# def preprocess_input(news):\n",
    "#     words = news.lower().split()\n",
    "#     encoded_review = [word_index.get(word, 2) for word in words]  # Using '2' for out-of-vocabulary words\n",
    "#     return pad_sequences([encoded_review], maxlen=max_len)\n",
    "\n",
    "# # Function to predict disaster-related news\n",
    "# def predict_news(news):\n",
    "#     preprocessed_text = preprocess_input(news)\n",
    "#     prediction = model.predict(preprocessed_text)\n",
    "#     sentiment = 'DisasterRelated' if prediction[0][0] > 0.5 else 'Not Related'\n",
    "#     return sentiment, prediction[0][0]\n",
    "\n",
    "# # Example usage\n",
    "# news = \"Earthquake shakes the city, people are trapped\"\n",
    "# result, score = predict_news(news)\n",
    "# print(f'Result: {result}')\n",
    "# print(f'Score: {score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f05edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\negia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\negia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\negia\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 63ms/step - accuracy: 0.6077 - loss: 2.2060 - val_accuracy: 0.7923 - val_loss: 0.5246 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - accuracy: 0.8775 - loss: 0.3618 - val_accuracy: 0.7865 - val_loss: 0.5257 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.9195 - loss: 0.2622 - val_accuracy: 0.7594 - val_loss: 0.6146 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.9586 - loss: 0.1616 - val_accuracy: 0.7709 - val_loss: 0.7042 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.9770 - loss: 0.1047 - val_accuracy: 0.7512 - val_loss: 0.8303 - learning_rate: 5.0000e-04\n",
      "Epoch 6/25\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.9837 - loss: 0.0808 - val_accuracy: 0.7594 - val_loss: 0.8598 - learning_rate: 5.0000e-04\n",
      "Epoch 7/25\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - accuracy: 0.9856 - loss: 0.0717 - val_accuracy: 0.7521 - val_loss: 1.0297 - learning_rate: 5.0000e-04\n",
      "Epoch 8/25\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 60ms/step - accuracy: 0.9873 - loss: 0.0591 - val_accuracy: 0.7438 - val_loss: 1.1033 - learning_rate: 2.5000e-04\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step\n",
      "Result: DisasterRelated\n",
      "Score: 0.73\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "# import nltk\n",
    "# import re\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     lemma = nltk.WordNetLemmatizer()\n",
    "#     text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
    "#     text = ' '.join([lemma.lemmatize(word) for word in text.split() if word not in nltk.corpus.stopwords.words('english')])\n",
    "#     return text\n",
    "\n",
    "\n",
    "# df = pd.read_csv('trainDisaster.csv')\n",
    "\n",
    "\n",
    "# df['text'] = df['text'].fillna('').apply(preprocess_text)\n",
    "# df['keyword'] = df['keyword'].fillna('unknown')\n",
    "# df['location'] = df['location'].fillna('unknown')\n",
    "\n",
    "\n",
    "# df['target'] = df['target'].astype(int)\n",
    "\n",
    "# # Tokenizer setup\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(df['text'])\n",
    "# word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "# X = tokenizer.texts_to_sequences(df['text'])\n",
    "# y = df['target']\n",
    "\n",
    "\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "# class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "# max_len = 50\n",
    "# X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# model = Sequential([\n",
    "#     Embedding(input_dim=len(word_index) + 1, output_dim=100, input_length=max_len),\n",
    "#     LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "#     Dropout(0.3),\n",
    "#     LSTM(64, kernel_regularizer=l2(0.01)),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# earlystopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)\n",
    "\n",
    "# model.fit(\n",
    "#     X_train, y_train,\n",
    "#     epochs=25,\n",
    "#     batch_size=32,\n",
    "#     validation_split=0.2,\n",
    "# #     class_weight=class_weights,\n",
    "#     callbacks=[earlystopping, reduce_lr]\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def preprocess_input(news):\n",
    "#     words = news.lower().split()\n",
    "#     encoded_review = [word_index.get(word, 2) for word in words]\n",
    "#     padded_review = pad_sequences([encoded_review], maxlen=max_len)\n",
    "#     return padded_review\n",
    "\n",
    "# def predict_news(news):\n",
    "#     preprocessed_text = preprocess_input(news)\n",
    "#     prediction = model.predict(preprocessed_text)[0][0]\n",
    "#     sentiment = 'DisasterRelated' if prediction > 0.4 else 'Not Related'\n",
    "#     return sentiment, prediction\n",
    "\n",
    "# news = \"Welcoming Mr. Jha to the part, the former Delhi CM said, “Anil Jha, who works for the backward people in Delhi will strengthent\"\n",
    "# result, score = predict_news(news)\n",
    "# print(f'Result: {result}')\n",
    "# print(f'Score: {score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6883adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Result: DisasterRelated\n",
      "Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "news = \"Welcoming Mr. Jha to the part, the former Delhi CM said, “Anil Jha, who works for the backward people in Delhi will strengthen\"\n",
    "result, score = predict_news(news)\n",
    "print(f'Result: {result}')\n",
    "print(f'Score: {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e89fa8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\negia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\negia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\negia\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 376ms/step - accuracy: 0.5602 - loss: 0.6810 - val_accuracy: 0.6560 - val_loss: 0.6454 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 364ms/step - accuracy: 0.6517 - loss: 0.6190 - val_accuracy: 0.7882 - val_loss: 0.4746 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 393ms/step - accuracy: 0.8967 - loss: 0.2901 - val_accuracy: 0.7742 - val_loss: 0.5640 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 389ms/step - accuracy: 0.9493 - loss: 0.1405 - val_accuracy: 0.7488 - val_loss: 0.8047 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 382ms/step - accuracy: 0.9906 - loss: 0.0352 - val_accuracy: 0.7438 - val_loss: 0.9083 - learning_rate: 2.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 394ms/step - accuracy: 0.9929 - loss: 0.0322 - val_accuracy: 0.7258 - val_loss: 0.9743 - learning_rate: 2.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Result: DisasterRelated\n",
      "Score: 0.65\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import nltk\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    lemma = nltk.WordNetLemmatizer()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text).lower()  # Remove non-alphabetic characters\n",
    "    text = ' '.join([lemma.lemmatize(word) for word in text.split() if word not in nltk.corpus.stopwords.words('english')])\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('trainDisaster.csv')\n",
    "\n",
    "# Fill missing values and preprocess text\n",
    "df['text'] = df['text'].fillna('').apply(preprocess_text)\n",
    "df['keyword'] = df['keyword'].fillna('unknown')\n",
    "df['location'] = df['location'].fillna('unknown')\n",
    "\n",
    "# Convert target to integers\n",
    "df['target'] = df['target'].astype(int)\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Prepare input and target data\n",
    "X = tokenizer.texts_to_sequences(df['text'])\n",
    "y = df['target']\n",
    "\n",
    "# Pad sequences\n",
    "max_len = 250  # Increased max_len to capture more context\n",
    "X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=df['text'].apply(lambda x: x.split()), vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Create embedding matrix from Word2Vec\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word_index) + 1, \n",
    "              output_dim=embedding_dim, \n",
    "              input_length=max_len, \n",
    "              weights=[embedding_matrix], \n",
    "              trainable=True),  # Allow fine-tuning Word2Vec embeddings\n",
    "    Bidirectional(LSTM(128, return_sequences=True, dropout=0.4, recurrent_dropout=0.3)),\n",
    "    Bidirectional(LSTM(64, dropout=0.4, recurrent_dropout=0.3)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,  # Increased epochs for better training\n",
    "    batch_size=32,  # Smaller batch size\n",
    "    validation_split=0.2,\n",
    "    callbacks=[earlystopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('disaster_rnn_model_optimized.h5')\n",
    "\n",
    "# Prediction function\n",
    "def preprocess_input(news):\n",
    "    words = preprocess_text(news).split()\n",
    "    encoded_review = [word_index.get(word, 2) for word in words]\n",
    "    padded_review = pad_sequences([encoded_review], maxlen=max_len)\n",
    "    return padded_review\n",
    "\n",
    "def predict_news(news):\n",
    "    preprocessed_text = preprocess_input(news)\n",
    "    prediction = model.predict(preprocessed_text)[0][0]\n",
    "    sentiment = 'DisasterRelated' if prediction > 0.5 else 'Not Related'\n",
    "    return sentiment, prediction\n",
    "\n",
    "# Example usage\n",
    "news = \"The flooding in the region has caused massive destruction.\"\n",
    "result, score = predict_news(news)\n",
    "print(f'Result: {result}')\n",
    "print(f'Score: {score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12507356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Result: Not Related\n",
      "Score: 0.38\n"
     ]
    }
   ],
   "source": [
    "news = \"Horoscope Today LIVE Updates on November 18, 2024 : Horoscope Today: Astrological prediction for November 18, 2024\"\n",
    "result, score = predict_news(news)\n",
    "print(f'Result: {result}')\n",
    "print(f'Score: {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f7578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
